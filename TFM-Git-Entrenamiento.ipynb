{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 id=\"Librer%C3%ADas-y-variables-de-entorno\">Inicializacion del Entorno<a class=\"anchor-link\" href=\"#Librer%C3%ADas-y-variables-de-entorno\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UOC\n",
    "# Máster de Ciencia de Datos \n",
    "# TFM - Análisis de posible cáncer de piel mediante IA\n",
    "# Enrique Fernández Morales\n",
    "\n",
    "#%pip freeze > requirements.txt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import asarray \n",
    "from datasets import load_dataset \n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from vit_keras import vit\n",
    "from tensorflow.keras.applications import EfficientNetB0, Xception, ResNet50, ResNet152, VGG16, DenseNet201, MobileNetV2, InceptionResNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import warnings\n",
    "from PIL import Image\n",
    "import csv\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# GPU CUDA\n",
    "# Directorio con las librerías CUDA\n",
    "#os.add_dll_directory('C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.2\\\\bin')\n",
    "# Obtención de información de la GPU dispponible\n",
    "from tensorflow.python.client import device_lib\n",
    "def print_info():\n",
    "    print('  Versión de TensorFlow: {}'.format(tf.__version__))\n",
    "    print('  GPU: {}'.format([x.physical_device_desc for x in device_lib.list_local_devices() if x.device_type=='GPU']))\n",
    "    print('  Versión Cuda  -> {}'.format(tf.sysconfig.get_build_info()['cuda_version']))\n",
    "    print('  Versión Cudnn -> {}\\n'.format(tf.sysconfig.get_build_info()['cudnn_version']))\n",
    "print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elije una configuracion (CF1, CF2, CF3, CF4, CF5) y maquela como True\n",
    "#######################\n",
    "CF = 'CF1'\n",
    "CF1_flag = True\n",
    "CF2_flag = False\n",
    "CF3_flag = False\n",
    "CF4_flag = False\n",
    "CF5_flag = False\n",
    "#######################\n",
    "\n",
    "#Especifique los directorios necesarios\n",
    "#######################\n",
    "path_or = '.\\\\Datos\\\\'\n",
    "path_img = path_or + f'{CF}\\\\imagenes\\\\'\n",
    "input_path = path_or + f'{CF}\\\\salidas\\\\'\n",
    "#######################\n",
    "\n",
    "sys.path.append('.\\\\swintransformer')\n",
    "from swintransformer import SwinTransformer\n",
    "\n",
    "# Dimensiones de las imágenes para entrenar el modelo adhoc\n",
    "WIDTH = 28\n",
    "HEIGHT = 28\n",
    "\n",
    "# Dimensiones de las imágenes para redes preentrenadas\n",
    "WIDTH_224 = 224\n",
    "HEIGHT_224 = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Elige que modelos entrenar y que modelos testear\n",
    "#######################\n",
    "train_model_cnn_adhoc_flag = False\n",
    "test_model_cnn_adhoc_flag = False\n",
    "\n",
    "train_model_vit_flag = False\n",
    "test_model_vit_flag = False\n",
    "\n",
    "train_model_ST_flag = False\n",
    "test_model_ST_flag = False\n",
    "\n",
    "train_model_EfficientNet_flag = False\n",
    "test_model_EfficientNet_flag = False\n",
    "\n",
    "train_model_xception_flag = False\n",
    "test_model_xception_flag = False\n",
    "\n",
    "train_model_resnet_flag = False\n",
    "test_model_resnet_flag = False\n",
    "\n",
    "train_model_VGG16_flag = False\n",
    "test_model_VGG16_flag = False\n",
    "\n",
    "train_model_densenet_flag = False\n",
    "test_model_densenet_flag = False\n",
    "\n",
    "train_model_mobilenet_flag = False\n",
    "test_model_mobilenet_flag = False\n",
    "\n",
    "train_model_InceptionResNetV2_flag = False\n",
    "test_model_InceptionResNetV2_flag = False\n",
    "#######################\n",
    "\n",
    "#Especificar si guardar la matriz de pesos\n",
    "#######################\n",
    "save_matriz_pesos_flag = True\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CF1_flag or CF4_flag or CF5_flag:\n",
    "    # Diccionario con las classes CF1\n",
    "    classes = {0: ('bcc', 'CARCINOMA'),\n",
    "               1: ('df', 'DERMATOFIBROMA'),\n",
    "               2: ('bkl', 'LENTIGO'),\n",
    "               3: ('vasc', 'LESION_VASCULAR'),\n",
    "               4: ('mel', 'MELANOMA'),\n",
    "               5: ('nv', 'NEVUS'),\n",
    "               6: ('akiec', 'QUERATOSIS')}\n",
    "\n",
    "    cm_plot_classes = ['bcc', 'df', 'bkl', 'vasc', 'mel', 'nv', 'akiec']\n",
    "\n",
    "if CF2_flag or CF3_flag:\n",
    "    # Diccionario con las classes CF2\n",
    "    classes = {0: ('ben', 'BENIGNO'),\n",
    "               1: ('mal', 'MALIGNO')}\n",
    "\n",
    "    cm_plot_classes = ['ben', 'mal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 id=\"An%C3%A1lisis-exploratoria-de-datos\">Análisis exploratoria de datos<a class=\"anchor-link\" href=\"#An%C3%A1lisis-exploratoria-de-datos\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(path_or + f'metadata_{CF}.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de frecuencia de imágenes de cada clase - CF1\n",
    "plt.figure(figsize = (12, 8))\n",
    "sns.set_theme(style = 'darkgrid')\n",
    "sns.countplot(data = metadata, x = 'label', order = metadata['label'].value_counts().index, palette = 'mako')\n",
    "plt.xlabel('Clase', size = 15)\n",
    "plt.ylabel('Cantidad', size = 15)\n",
    "plt.title('Distribución de frecuencia de imágenes de cada clase', size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 id=\"Funciones\">Funciones<a class=\"anchor-link\" href=\"#Funciones\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para la representación gráfica deL accuracy y loss de los conjuntos de entrenamiento y validación\n",
    "def plot_acc_loss(histfit):\n",
    "    \"\"\"\n",
    "    Función que representa las gráficas de accuracy y loss de los conjuntos de entrenamiento y validación.\n",
    "    \n",
    "    Parámetro de entrada:\n",
    "    histfit: histórico del entrenamiento y validación\n",
    "    \"\"\"\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4)) # 2 gráficas en la misma fila \n",
    "    fig.suptitle(f'{CF} - Accuracy y Loss del modelo')\n",
    "    sns.set_theme(style = 'darkgrid')\n",
    "    \n",
    "    # accuracy\n",
    "    ax1.plot(histfit['accuracy'])\n",
    "    ax1.plot(histfit['val_accuracy'])\n",
    "    ax1.set(xlabel = 'Epoch', ylabel = 'Accuracy')\n",
    "    ax1.legend(['train', 'valid'], loc = 'lower right')\n",
    "       \n",
    "    # loss\n",
    "    ax2.plot(histfit['loss'])\n",
    "    ax2.plot(histfit['val_loss'])\n",
    "    ax2.set(xlabel = 'Epoch', ylabel = 'Loss')\n",
    "    ax2.legend(['train','valid'], loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para representar la matriz de confusión\n",
    "def plot_confusion_matrix(cm, classes, normalize = False, cmap = plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Función que representa la matriz de confusión.\n",
    "    Se puede obtener normalizada en porcentajes con: normalize = True.\n",
    "    \n",
    "    Parámetros de entrada:\n",
    "    cm: matriz de confusión\n",
    "    classes: array con las clases\n",
    "    normalize: boleano para indicar si se quieren los datos normalizados o no\n",
    "    cmap: paleta de colores\n",
    "    \n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "        title = f'{CF} - Matriz de confusión normalizada en porcentajes'\n",
    "    else:\n",
    "        title = f'{CF} - Matriz de confusión sin normalizar'\n",
    "        \n",
    "    plt.figure(figsize = (10,8))\n",
    "    plt.title(title, size = 18)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 45, size = 13)\n",
    "    plt.yticks(tick_marks, classes, size = 13)\n",
    "    fmt = '.2%' if normalize else 'd'\n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                     index = classes, \n",
    "                     columns = classes)\n",
    "    sns.heatmap(cm_df, annot = True, fmt = fmt, cmap = cmap)\n",
    "    plt.ylabel('Realidad', size = 15)\n",
    "    plt.xlabel('Predicción', size = 15)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para representar la curva ROC multiclase\n",
    "def plot_multiclass_roc(model, X_test, y_test, classes, n_classes, figsize = (14, 7)):\n",
    "    \"\"\"\n",
    "    Función que devuelve una representación gráfica de la curva ROC multiclase.\n",
    "    \n",
    "    Parámetros de entrada:\n",
    "    model: modelo con el que se realiza la predicción\n",
    "    X_test: características del conjunto de test\n",
    "    y_test: etiquetas del conjunto de test\n",
    "    n_classes: entero con el número de clases\n",
    "    figsize: dimensiones del gráfico\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predicción\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Diccionarios\n",
    "    fpr = dict()     # ratio de FP\n",
    "    tpr = dict()     # ratio de TP\n",
    "    roc_auc = dict() # curva ROC\n",
    "    \n",
    "    # Codificación one hot\n",
    "    y_test_dummies = pd.get_dummies(y_test, drop_first = False).values\n",
    "    \n",
    "    # Iteración por todas las clases\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = metrics.roc_curve(y_test_dummies[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "        \n",
    "    # ROC para cada clase\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('Ratio de falsos positivos')\n",
    "    ax.set_ylabel('Ratio de verdaderos positivos')\n",
    "    ax.set_title(f'{CF} - CURVA ROC')\n",
    "    if CF1_flag or CF4_flag or CF5_flag:\n",
    "        for i in range(n_classes):\n",
    "            ax.plot(fpr[i], tpr[i], label = 'Curva ROC (area = %0.2f) para la lesión cutánea %s' % (roc_auc[i], classes[i][1]))\n",
    "    if CF2_flag or CF3_flag:\n",
    "        ax.plot(fpr[0], tpr[0], label = 'Curva ROC (area = %0.2f) para la lesión cutánea %s' % (roc_auc[0], classes[0][1]))\n",
    "        ax.plot(fpr[1], tpr[1], label = 'Curva ROC (area = %0.2f) para la lesión cutánea %s' % (roc_auc[1], classes[1][1]))\n",
    "    ax.legend(loc = \"best\")\n",
    "    ax.grid(alpha = .4)\n",
    "    sns.despine()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que entrena, valida y guarda los resultados en el diccionario histfit\n",
    "def fn_train_val(histfit, model, x, y, batch_size, epochs, callbacks, x_val, y_val):\n",
    "    \"\"\"\n",
    "    Función que entrena y valida el modelo con el conjunto de datos, número de épocas y tamaño del batch introducidos\n",
    "    como parámetros de entrada.\n",
    "\n",
    "    histfit: diccionario para guardar los resultados del entrenamiento y validación\n",
    "    model: modelo a entrenar\n",
    "    x: conjunto de entrenamiento\n",
    "    y: etiquetas del conjunto de entrenamiento\n",
    "    batch_size: tamaño del batch\n",
    "    epochs: número de épocas   \n",
    "    callbacks: devolución de llamadas\n",
    "    x_val: \n",
    "    y_val = \n",
    "\n",
    "    \n",
    "    Devuelve:\n",
    "    histfit -- diccionario con los resultados del entrenamiento y validación\n",
    "    \"\"\"\n",
    "    \n",
    "    history = model.fit(x = x, y = y, batch_size = batch_size, epochs = epochs,\n",
    "                            callbacks = callbacks, validation_data=(x_val, y_val))\n",
    "\n",
    "    # Adición de los resultados al diccionario\n",
    "    for key, value in history.history.items():\n",
    "        for i in range(len(value)):\n",
    "            if key != 'lr':\n",
    "                histfit[key].append(value[i])\n",
    "    return histfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que entrena, valida y guarda los resultados en el diccionario histfit\n",
    "def fn_train_val2(histfit, model, x, y, batch_size, epochs, callbacks, steps_per_epoch = None, validation_steps = None, validation_batch_size = None):\n",
    "    \"\"\"\n",
    "    Función que entrena y valida el modelo con el conjunto de datos, número de épocas y tamaño del batch introducidos\n",
    "    como parámetros de entrada.\n",
    "\n",
    "    histfit: diccionario para guardar los resultados del entrenamiento y validación\n",
    "    model: modelo a entrenar\n",
    "    x: conjunto de entrenamiento\n",
    "    y: etiquetas del conjunto de entrenamiento\n",
    "    batch_size: tamaño del batch\n",
    "    epochs: número de épocas   \n",
    "    callbacks: devolución de llamadas\n",
    "    steps_per_epoch: pasos por época\n",
    "    validation_steps: pasos en la validación\n",
    "    validation_batch_size: tamaño del batch en la validación\n",
    "\n",
    "    \n",
    "    Devuelve:\n",
    "    histfit -- diccionario con los resultados del entrenamiento y validación\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    history = model.fit(x = x, validation_data = y, batch_size = batch_size,\n",
    "                            epochs = epochs, callbacks = callbacks, steps_per_epoch = steps_per_epoch, \n",
    "                            validation_steps = validation_steps, validation_batch_size = validation_batch_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Adición de los resultados al diccionario\n",
    "    for key, value in history.history.items():\n",
    "        for i in range(len(value)):\n",
    "            if key != 'lr':\n",
    "                histfit[key].append(value[i])\n",
    "    return histfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de clasificación de una imagen con un modelo determinado\n",
    "def class_img(model_key, image):\n",
    "    \"\"\"\n",
    "    Función que clasifica la imagen introducida con el modelo seleccionado.\n",
    "    \n",
    "    Parámetros de entrada:\n",
    "    model_key: clave del modelo. Diccionario: saved_models.\n",
    "    image: nombre del fichero que contiene la imagen a clasificar.\n",
    "    \n",
    "    Devuelve la clase predicha y el porcentaje de acierto de la clasificación.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Asignación del modelo\n",
    "    model = saved_models[model_key]\n",
    "        \n",
    "    # Ruta completa de la imagen a clasificar\n",
    "    img_path = image\n",
    "        \n",
    "    if model_key == 1:\n",
    "        width = WIDTH\n",
    "        height = HEIGHT\n",
    "    else:\n",
    "        width = WIDTH_224\n",
    "        height = HEIGHT_224\n",
    "        \n",
    "    # Obtención de la matriz de características de la imagen\n",
    "    img_arr = get_array_from_img(img_path, width, height)\n",
    "    # Dimensionamiento del conjunto de datos a las dimensiones esperada por el modelo\n",
    "    x = np.array(img_arr).reshape(-1, width, height, 3)\n",
    "            \n",
    "    # Normalización\n",
    "    x = (x - np.mean(x)) / np.std(x)\n",
    "        \n",
    "    # Clasificación de la imagen\n",
    "    result = model.predict(x)\n",
    "    class_pred = max(result[0])                                        # Clase predicha\n",
    "    class_ind = list(result[0]).index(class_pred)                      # Índice de la clase predicha\n",
    "    class_name = classes[class_ind][0] + ' · ' + classes[class_ind][1] # Descriptivo de la clase predicha\n",
    "    print('***** CLASIFICANDO CON MODELO ' + str(model_key) + '/' + str(len(saved_models)) + ' *****\\n')\n",
    "    print('\\tResultado de la clasificación: ' + class_name)\n",
    "    print('\\n\\tCerteza del resultado: ' + str(round(class_pred * 100, 3)) + '%\\n')\n",
    "    \n",
    "    return class_name, round(class_pred * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para la obtención de la matriz de características de una imagen\n",
    "def get_array_from_img (img_path, width, height):\n",
    "    \"\"\"\n",
    "    Función que lee la imagen introducida como parámetro de entrada y la reescala a las dimensiones introducidas.\n",
    "    Devuelve una matriz de dimensiones width x height x 3 (RGB) con las características de la imagen.\n",
    "    \n",
    "    Parámetros de entrada:\n",
    "    img_path: ruta completa (incluido el nombre del fichero) de la imagen de entrada\n",
    "    width: ancho de la imagen de salida\n",
    "    height: alto de la imagen de salida\n",
    "    \n",
    "    Devuelve:\n",
    "    img_resized: matriz con las características de la imagen de entrada con dimensiones width x height x 3\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)        # lectura y conversión de formato BGR a RGB\n",
    "        dim = (width, height)\n",
    "        img_resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA) # reescalado de la imagen\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return img_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función con la configuración de callbacks\n",
    "def def_callbacks(e_monitor = 'val_loss', e_mode = 'min', e_patience = 5, e_verbose = 2,\n",
    "                  r_monitor = 'val_accuracy', r_factor = 0.2, r_patience = 2, r_min_delta = 1e-4,\n",
    "                  r_min_lr = 1e-6, r_mode = 'max', r_verbose = 2, c_filepath = 'best_model.h5', \n",
    "                  c_monitor = 'val_accuracy', c_save_best_only = True, c_save_weights_only = True,\n",
    "                  c_mode = 'max', c_verbose = 2):\n",
    "    \"\"\"\n",
    "    Función que devuelve una lista con las opciones de callbacks\n",
    "    \n",
    "    Devuelve:\n",
    "    callbacks: lista con la configuración de la parada temprana, caida de la tasa de aprendizaje y punto de control\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Parada temprana en el caso de que la pérdida en la validación no mejore en 5 épocas\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = e_monitor,\n",
    "                                                      mode = e_mode,\n",
    "                                                      patience = e_patience,\n",
    "                                                      verbose = e_verbose)\n",
    "    \n",
    "    # Caida de la tasa de aprendizaje después de 2 épocas sin mejora en el accuracy de la validación\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = r_monitor,\n",
    "                                                     factor = r_factor,\n",
    "                                                     patience = r_patience,\n",
    "                                                     min_delta = r_min_delta,\n",
    "                                                     min_lr = r_min_lr,\n",
    "                                                     mode = r_mode,\n",
    "                                                     verbose = r_verbose)\n",
    "    \n",
    "    # Guardado de los pesos del mejor modelo en función del mejor accuracy en la validación\n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = c_filepath,\n",
    "                                                      monitor = c_monitor,\n",
    "                                                      save_best_only = c_save_best_only,\n",
    "                                                      save_weights_only = c_save_weights_only,\n",
    "                                                      mode = c_mode,\n",
    "                                                      verbose = c_verbose)\n",
    "    \n",
    "    # Array con los callbacks\n",
    "    callbacks = [early_stopping, checkpointer, reduce_lr]\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 id=\"Entrenamientos\">Entrenamientos<a class=\"anchor-link\" href=\"#Entrenamientos\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz para guardar los pesos del accuracy de cada modelo para cada clase\n",
    "if CF1_flag or CF4_flag or CF5_flag:\n",
    "    m1 = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "\n",
    "if CF2_flag or CF3_flag:\n",
    "    m1 = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2 id=\"Modelo-1-%C2%B7-CNN-adhoc\">Modelo 1 · CNN adhoc<a class=\"anchor-link\" href=\"#Modelo-1-%C2%B7-CNN-adhoc\">¶</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de los conjuntos de train y test\n",
    "x_train = np.load(input_path + 'x_train_adhoc.npy')\n",
    "x_test = np.load(input_path + 'x_test_adhoc.npy')\n",
    "y_train = np.load(input_path + 'y_train_adhoc.npy')\n",
    "y_test = np.load(input_path + 'y_test_adhoc.npy')\n",
    "x_val = np.load(input_path + 'x_val_adhoc.npy')\n",
    "y_val = np.load(input_path + 'y_val_adhoc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de creación del modelo CNN adhoc\n",
    "def build_model_cnn_adhoc():\n",
    "    \"\"\"\n",
    "    Crea una red neuronal convolucional con 4 capas convolucionales de 16, 32, 32 y 64 neuronas (kernels) de 3x3 y función de activación ReLU,\n",
    "    2 capes de MaxPooling 2D de 2x2,\n",
    "    1 capa Flatten que conecta la salida de la capa convolucional con una entrada de la capa densa,\n",
    "    2 capas densas de 64 y 32 neuronas completamente conectadas y activación ReLU y\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, kernel_size = (3,3), input_shape = (28, 28, 3), activation = 'relu', padding = 'same')) # capa oculta convolucional con 16 kernels de 3x3 y entrada (28, 28, 3)\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu'))                                              # capa oculta convolucional con 32 kernels de 3x3\n",
    "    model.add(MaxPool2D(pool_size = (2,2)))                                                                      # capa de MaxPooling 2D\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu', padding = 'same'))                            # capa oculta convolucional con 32 kernels de 3x3\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))                                              # capa oculta convolucional con 64 kernels de 3x3\n",
    "    model.add(MaxPool2D(pool_size = (2,2), padding = 'same'))                                                    # capa de MaxPooling 2D\n",
    "    model.add(Flatten())                                                                                         # capa de Flatten\n",
    "    model.add(Dense(64, activation='relu'))                                                                      # capa densa completamente conectada de 64 neuronas\n",
    "    model.add(Dense(32, activation='relu'))                                                                      # capa densa completamente conectada de 32 neuronas\n",
    "    model.add(Dense(7, activation='softmax'))                                                                    # capa de clasificación con función Softmax para 7 clases\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Función de creación del modelo CNN adhoc binario\n",
    "def build_model_cnn_adhoc_binary():\n",
    "    \"\"\"\n",
    "    Crea una red neuronal convolucional con 4 capas convolucionales de 16, 32, 32 y 64 neuronas (kernels) de 3x3 y función de activación ReLU,\n",
    "    2 capes de MaxPooling 2D de 2x2,\n",
    "    1 capa Flatten que conecta la salida de la capa convolucional con una entrada de la capa densa,\n",
    "    2 capas densas de 64 y 32 neuronas completamente conectadas y activación ReLU y\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, kernel_size = (3,3), input_shape = (28, 28, 3), activation = 'relu', padding = 'same')) # capa oculta convolucional con 16 kernels de 3x3 y entrada (28, 28, 3)\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu'))                                              # capa oculta convolucional con 32 kernels de 3x3\n",
    "    model.add(MaxPool2D(pool_size = (2,2)))                                                                      # capa de MaxPooling 2D\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu', padding = 'same'))                            # capa oculta convolucional con 32 kernels de 3x3\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))                                              # capa oculta convolucional con 64 kernels de 3x3\n",
    "    model.add(MaxPool2D(pool_size = (2,2), padding = 'same'))                                                    # capa de MaxPooling 2D\n",
    "    model.add(Flatten())                                                                                         # capa de Flatten\n",
    "    model.add(Dense(64, activation='relu'))                                                                      # capa densa completamente conectada de 64 neuronas\n",
    "    model.add(Dense(32, activation='relu'))                                                                      # capa densa completamente conectada de 32 neuronas\n",
    "    model.add(Dense(2, activation = 'softmax'))                                                                    # capa de clasificación con función sigmoid para 1 clases\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Función de creación del modelo CNN adhoc Dropout\n",
    "def build_model_cnn_adhoc_dropout():\n",
    "    \"\"\"\n",
    "    Crea una red neuronal convolucional con 4 capas convolucionales de 16, 32, 32 y 64 neuronas (kernels) de 3x3 y función de activación ReLU,\n",
    "    2 capes de MaxPooling 2D de 2x2,\n",
    "    1 capa Flatten que conecta la salida de la capa convolucional con una entrada de la capa densa,\n",
    "    2 capas densas de 64 y 32 neuronas completamente conectadas y activación ReLU y\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, kernel_size = (3,3), input_shape = (28, 28, 3), activation = 'relu', padding = 'same')) # capa oculta convolucional con 16 kernels de 3x3 y entrada (28, 28, 3)\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu'))                                              # capa oculta convolucional con 32 kernels de 3x3\n",
    "    model.add(MaxPool2D(pool_size = (2,2)))                                                                      # capa de MaxPooling 2D\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu', padding = 'same'))                            # capa oculta convolucional con 32 kernels de 3x3\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))                                              # capa oculta convolucional con 64 kernels de 3x3\n",
    "    model.add(MaxPool2D(pool_size = (2,2), padding = 'same'))                                                    # capa de MaxPooling 2D\n",
    "    model.add(Flatten())                                                                                         # capa de Flatten\n",
    "    model.add(Dense(64, activation='relu'))                                                                      # capa densa completamente conectada de 64 neuronas\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))                                                                      # capa densa completamente conectada de 32 neuronas\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(7, activation='softmax'))                                                                    # capa de clasificación con función Softmax para 7 clases\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_cnn_adhoc_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model_adhoc = build_model_cnn_adhoc()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model_adhoc = build_model_cnn_adhoc_binary()\n",
    "    if CF5_flag:\n",
    "        model_adhoc = build_model_cnn_adhoc_dropout()\n",
    "    model_adhoc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_cnn_adhoc_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32, 64, 128]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    if CF5_flag:\n",
    "        # Hiperparámetros\n",
    "        # epochs\n",
    "        n_epochs = [25, 50]\n",
    "        # learning rates\n",
    "        learning_rates = [0.001, 0.0005, 0.0001, 0.00005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_cnn_adhoc_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history1 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo CNN adhoc...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            #train_dataset, valid_dataset = img_aumentation(x_train_adhoc, y_train_adhoc, x_test_adhoc, y_test_adhoc, batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_cnn_adhoc()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_cnn_adhoc_binary()\n",
    "                if CF5_flag:\n",
    "                    model = build_model_cnn_adhoc_dropout()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                \n",
    "                model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "                \n",
    "\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val(histfit, model, x_train, y_train, batch_size, epochs,'+\n",
    "                                             'callbacks, x_val, y_val)', globals = globals(), number = 1)\n",
    "\n",
    "                # Adición de los resultados al diccionario history1\n",
    "                history1['epochs'].append(epochs)\n",
    "                history1['batch_size'].append(batch_size)\n",
    "                history1['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history1['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history1['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history1['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history1['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history1['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(input_path + 'df_histfit1')\n",
    "                    model.save(input_path + 'cnn_adhoc_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history1 = pd.DataFrame.from_dict(history1)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history1.to_pickle(input_path + 'df_history1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "df_history1 = pd.read_pickle(input_path + 'df_history1')\n",
    "\n",
    "# Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "df_history1.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "df_histfit1 = pd.read_pickle(input_path + 'df_histfit1')\n",
    "\n",
    "# Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "plot_acc_loss(df_histfit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_cnn_adhoc_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model_adhoc = build_model_cnn_adhoc()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model_adhoc = build_model_cnn_adhoc_binary()\n",
    "    if CF5_flag:\n",
    "        model_adhoc = build_model_cnn_adhoc_dropout()\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(input_path + 'cnn_adhoc_best_model.h5')\n",
    "    model.save(input_path + 'model1.h5') # Guardado completo del modelo\n",
    "\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose = 2)   # Evaluación del modelo con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del modelo entero\n",
    "if CF1_flag or CF4_flag:\n",
    "    model = build_model_cnn_adhoc()\n",
    "if CF2_flag or CF3_flag:\n",
    "    model = build_model_cnn_adhoc_binary()\n",
    "if CF5_flag:\n",
    "    model = build_model_cnn_adhoc_dropout()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.load_weights(input_path + 'model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción con el conjunto de test\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC multiclase\n",
    "plot_multiclass_roc(model, x_test, y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "\n",
    "# Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "#plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print('***** MÉTRICAS *****')\n",
    "print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adición a la matriz m1 del accuracy del modelo para cada clase\n",
    "for i in range(len(classes)):\n",
    "    m1['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Modelo-2-%C2%B7-Red-transformer-ViT-B16\">Modelo 2 · Red transformer ViT-B16<a class=\"anchor-link\" href=\"#Modelo-2-%C2%B7-Red-transformer-ViT-B16\">¶</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de los conjuntos de train y test\n",
    "x_train = np.load(input_path + 'x_train.npy')\n",
    "x_test = np.load(input_path + 'x_test.npy')\n",
    "y_train = np.load(input_path + 'y_train.npy')\n",
    "y_test = np.load(input_path + 'y_test.npy')\n",
    "x_val = np.load(input_path + 'x_val.npy')\n",
    "y_val = np.load(input_path + 'y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de creación del modelo Visión Transformer ViT-B/16\n",
    "def build_model_vit():\n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red Vision Transformer con arquitectura ViT-B/16 y\n",
    "    preentrenada con el dataset imagenet 21K, al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    2 capas de Flatten,\n",
    "    2 capas de normalización del batch,\n",
    "    1 capa densa completamente conectadad con activación gausiana y\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    # Importación del modelo ViT · B16 preentrenado y sin la capa de clasificación\n",
    "    vit_model = vit.vit_b16(image_size = 224,\n",
    "                            activation = 'softmax',\n",
    "                            pretrained = True,\n",
    "                            include_top = False,\n",
    "                            pretrained_top = False,\n",
    "                            classes = len(classes))\n",
    "    \n",
    "    # Creación del modelo con las nuevas capas para la clasificación\n",
    "    model = tf.keras.Sequential([vit_model,\n",
    "                                 keras.layers.Flatten(),                                    # capa de Flatten\n",
    "                                 keras.layers.BatchNormalization(),                         # capa de normalización del batch\n",
    "                                 keras.layers.Dense(11, activation = tfa.activations.gelu), # capa densa completamente contectada con activación gausiana\n",
    "                                 keras.layers.BatchNormalization(),                         # capa de normalización del batch\n",
    "                                 keras.layers.Dense(7, 'softmax')],                         # capa de clasificación con función Softmax para 7 clases\n",
    "                                name = 'vit_16')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Función de creación del modelo Visión Transformer ViT-B/16 binario\n",
    "\n",
    "def build_model_vit_binary():\n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red Vision Transformer con arquitectura ViT-B/16 y\n",
    "    preentrenada con el dataset imagenet 21K, al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    2 capas de Flatten,\n",
    "    2 capas de normalización del batch,\n",
    "    1 capa densa completamente conectadad con activación gausiana y\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    # Importación del modelo ViT · B16 preentrenado y sin la capa de clasificación\n",
    "    vit_model = vit.vit_b16(image_size = 224,\n",
    "                            activation = 'softmax',\n",
    "                            pretrained = True,\n",
    "                            include_top = False,\n",
    "                            pretrained_top = False,\n",
    "                            classes = len(classes))\n",
    "    \n",
    "    # Creación del modelo con las nuevas capas para la clasificación\n",
    "    model = tf.keras.Sequential([vit_model,\n",
    "                                 keras.layers.Flatten(),                                    # capa de Flatten\n",
    "                                 keras.layers.BatchNormalization(),                         # capa de normalización del batch\n",
    "                                 keras.layers.Dense(11, activation = tfa.activations.gelu), # capa densa completamente contectada con activación gausiana\n",
    "                                 keras.layers.BatchNormalization(),                         # capa de normalización del batch\n",
    "                                 keras.layers.Dense(2, activation = 'softmax')],              # capa de clasificación con función sigmoid para 1 clases\n",
    "                                name = 'vit_16')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Función de creación del modelo Visión Transformer ViT-B/16 Dropout\n",
    "def build_model_vit_dropout():\n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red Vision Transformer con arquitectura ViT-B/16 y\n",
    "    preentrenada con el dataset imagenet 21K, al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    2 capas de Flatten,\n",
    "    2 capas de normalización del batch,\n",
    "    1 capa densa completamente conectadad con activación gausiana y\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    # Importación del modelo ViT · B16 preentrenado y sin la capa de clasificación\n",
    "    vit_model = vit.vit_b16(image_size = 224,\n",
    "                            activation = 'softmax',\n",
    "                            pretrained = True,\n",
    "                            include_top = False,\n",
    "                            pretrained_top = False,\n",
    "                            classes = len(classes))\n",
    "    \n",
    "    # Creación del modelo con las nuevas capas para la clasificación\n",
    "    model = tf.keras.Sequential([vit_model,\n",
    "                                 keras.layers.Flatten(),                                    # capa de Flatten\n",
    "                                 keras.layers.BatchNormalization(),                         # capa de normalización del batch\n",
    "                                 Dropout(0.5),\n",
    "                                 keras.layers.Dense(11, activation = tfa.activations.gelu), # capa densa completamente contectada con activación gausiana\n",
    "                                 keras.layers.BatchNormalization(),                         # capa de normalización del batch\n",
    "                                 Dropout(0.5),\n",
    "                                 keras.layers.Dense(7, 'softmax')],                         # capa de clasificación con función Softmax para 7 clases\n",
    "                                name = 'vit_16')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_vit_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_vit()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_vit_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_vit_dropout()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_vit_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32, 64, 128]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    if CF5_flag:\n",
    "        # Hiperparámetros\n",
    "        # epochs\n",
    "        n_epochs = [25, 50]\n",
    "        # learning rates\n",
    "        learning_rates = [0.001, 0.0005, 0.0001, 0.00005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_vit_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history2 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # CallbacKs\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo Vision Transformer ViT...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_vit()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_vit_binary()\n",
    "                if CF5_flag:\n",
    "                    model = build_model_vit_dropout()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate) \n",
    "                model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "                \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, train_dataset, valid_dataset, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)        \n",
    "\n",
    "\n",
    "                # Adición de los resultados al diccionario history2\n",
    "                history2['epochs'].append(epochs)\n",
    "                history2['batch_size'].append(batch_size)\n",
    "                history2['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history2['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history2['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history2['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history2['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history2['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(input_path + 'df_histfit2')\n",
    "                    model.save(input_path + 'vit_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history2 = pd.DataFrame.from_dict(history2)\n",
    "\n",
    "    # Guardado del dataframe con todos los resultados de los entrenamientos\n",
    "    df_history2.to_pickle(input_path + 'df_history2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "df_history2 = pd.read_pickle(input_path + 'df_history2')\n",
    "\n",
    "# Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "df_history2.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "df_histfit2 = pd.read_pickle(input_path + 'df_histfit2')\n",
    "\n",
    "# Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "plot_acc_loss(df_histfit2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_vit_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_vit()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_vit_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_vit_dropout()\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(input_path + 'vit_best_model.h5')\n",
    "    model.save(input_path + 'model2.h5') # Guardado completo del modelo\n",
    "\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose = 2) # Evaluación del modelo con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del modelo completo\n",
    "if CF1_flag or CF4_flag:\n",
    "    model = build_model_vit()\n",
    "if CF2_flag or CF3_flag:\n",
    "    model = build_model_vit_binary()\n",
    "if CF5_flag:\n",
    "    model = build_model_vit_dropout()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.load_weights(input_path + 'model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción con el conjunto de test\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC multiclase\n",
    "plot_multiclass_roc(model, x_test, y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "\n",
    "# Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "#plot_confusion_matrix(cm, cm_plot_classes, normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print('***** MÉTRICAS *****')\n",
    "print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "if CF1_flag or CF4_flag or CF5_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "if CF2_flag or CF3_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "for i in range(len(classes)):\n",
    "    m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "    \n",
    "# Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "m1 = m1.append(m_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Modelo-3-%C2%B7-Swin-Transformer\">Modelo 3 · Swin Transformer<a class=\"anchor-link\" href=\"#Modelo-3-%C2%B7-Swin-Transformer\">¶</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de los conjuntos de train y test\n",
    "x_train = np.load(input_path + 'x_train.npy')\n",
    "x_test = np.load(input_path + 'x_test.npy')\n",
    "y_train = np.load(input_path + 'y_train.npy')\n",
    "y_test = np.load(input_path + 'y_test.npy')\n",
    "x_val = np.load(input_path + 'x_val.npy')\n",
    "y_val = np.load(input_path + 'y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de creación del modelo Swin Transformer\n",
    "def build_model_SwinT():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red transformer que utiliza la red Swin Transformer, preentrenada con el dataset ImageNet-22K,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal transformer\n",
    "    \"\"\"\n",
    "    \n",
    "    img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.imagenet_utils.preprocess_input(tf.cast(data, tf.float32), mode = 'torch'), input_shape = [224, 224, 3])\n",
    "    pretrained_model = SwinTransformer('swin_large_224',\n",
    "                                       num_classes = 7,\n",
    "                                       include_top = False,\n",
    "                                       pretrained = True,\n",
    "                                       use_tpu = False)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        img_adjust_layer,\n",
    "        pretrained_model,\n",
    "        keras.layers.Dense(7, activation = 'softmax')\n",
    "    ])\n",
    " \n",
    "    return model\n",
    "# Función de creación del modelo Swin Transformer binary\n",
    "def build_model_SwinT_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red transformer que utiliza la red Swin Transformer, preentrenada con el dataset ImageNet-22K,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal transformer\n",
    "    \"\"\"\n",
    "    \n",
    "    img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.imagenet_utils.preprocess_input(tf.cast(data, tf.float32), mode = 'torch'), input_shape = [224, 224, 3])\n",
    "    pretrained_model = SwinTransformer('swin_large_224',\n",
    "                                       num_classes = 7,\n",
    "                                       include_top = False,\n",
    "                                       pretrained = True,\n",
    "                                       use_tpu = False)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        img_adjust_layer,\n",
    "        pretrained_model,\n",
    "        keras.layers.Dense(2, activation = 'softmax')\n",
    "    ])\n",
    " \n",
    "    return model\n",
    "\n",
    "def build_model_SwinT_dropout():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red transformer que utiliza la red Swin Transformer, preentrenada con el dataset ImageNet-22K,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal transformer\n",
    "    \"\"\"\n",
    "    \n",
    "    img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.imagenet_utils.preprocess_input(tf.cast(data, tf.float32), mode = 'torch'), input_shape = [224, 224, 3])\n",
    "    pretrained_model = SwinTransformer('swin_large_224',\n",
    "                                       num_classes = 7,\n",
    "                                       include_top = False,\n",
    "                                       pretrained = True,\n",
    "                                       use_tpu = False)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        img_adjust_layer,\n",
    "        pretrained_model,\n",
    "        Dropout(0.5),\n",
    "        keras.layers.Dense(7, activation = 'softmax')\n",
    "    ])\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_ST_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_SwinT()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_SwinT_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_SwinT_dropout()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_ST_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [2]\n",
    "    # learning rates\n",
    "    learning_rates = [0.00001, 0.000005]\n",
    "    if CF5_flag:\n",
    "        # Hiperparámetros\n",
    "        # epochs\n",
    "        n_epochs = [25]\n",
    "        # learning rates\n",
    "        learning_rates = [0.00001, 0.000005, 0.000001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_ST_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history3 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo Swin Transformer...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            \n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_SwinT()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_SwinT_binary()\n",
    "                if CF5_flag:\n",
    "                    model = build_model_SwinT_dropout()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate) \n",
    "                model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy']) \n",
    "                            \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, train_dataset, valid_dataset, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)     \n",
    "                \n",
    "                # Adición de los resultados al diccionario history3\n",
    "                history3['epochs'].append(epochs)\n",
    "                history3['batch_size'].append(batch_size)\n",
    "                history3['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history3['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history3['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history3['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history3['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history3['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(input_path + 'df_histfit3')\n",
    "                    model.save(input_path + 'swin_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history3 = pd.DataFrame.from_dict(history3)\n",
    "\n",
    "    # Guardado del dataframe con todos los resultados de los entrenamientos\n",
    "    df_history3.to_pickle(input_path + 'df_history3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "df_history3 = pd.read_pickle(input_path + 'df_history3')\n",
    "\n",
    "# Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "df_history3.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "df_histfit3 = pd.read_pickle(input_path + 'df_histfit3')\n",
    "\n",
    "# Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "plot_acc_loss(df_histfit3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_ST_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_SwinT()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_SwinT_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_SwinT_dropout()\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(input_path + 'swin_best_model.h5')\n",
    "    model.save(input_path + 'model3.h5') # Guardado completo del modelo\n",
    "\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del modelo completo\n",
    "if CF1_flag or CF4_flag:\n",
    "    model = build_model_SwinT()\n",
    "if CF2_flag or CF3_flag:\n",
    "    model = build_model_SwinT_binary()\n",
    "if CF5_flag:\n",
    "    model = build_model_SwinT_dropout()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.load_weights(input_path + 'model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción con el conjunto de test\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC multiclase\n",
    "plot_multiclass_roc(model, x_test, y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "\n",
    "# Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "#plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print('***** MÉTRICAS *****')\n",
    "print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "if CF1_flag or CF4_flag or CF5_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "if CF2_flag or CF3_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "for i in range(len(classes)):\n",
    "    m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "    \n",
    "# Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "m1 = m1.append(m_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Modelo-4-%C2%B7-Red-EfficientNet-B0\">Modelo 4 · Red EfficientNet B0<a class=\"anchor-link\" href=\"#Modelo-4-%C2%B7-Red-EfficientNet-B0\">¶</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de los conjuntos de train y test\n",
    "x_train = np.load(input_path + 'x_train.npy')\n",
    "x_test = np.load(input_path + 'x_test.npy')\n",
    "y_train = np.load(input_path + 'y_train.npy')\n",
    "y_test = np.load(input_path + 'y_test.npy')\n",
    "x_val = np.load(input_path + 'x_val.npy')\n",
    "y_val = np.load(input_path + 'y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_EfficientNet():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = EfficientNetB0(include_top = False,                         # no incluir la capa de clasificación\n",
    "                           input_tensor = inputs, \n",
    "                           weights = 'imagenet')                        # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_EfficientNet_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = EfficientNetB0(include_top = False,                         # no incluir la capa de clasificación\n",
    "                           input_tensor = inputs, \n",
    "                           weights = 'imagenet')                        # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(2, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "\n",
    "def build_model_EfficientNet_dropout():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = EfficientNetB0(include_top = False,                         # no incluir la capa de clasificación\n",
    "                           input_tensor = inputs, \n",
    "                           weights = 'imagenet')                        # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5), \n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_EfficientNet_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_EfficientNet()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_EfficientNet_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_EfficientNet_dropout()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_EfficientNet_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32, 64, 128]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    if CF5_flag:\n",
    "        # Hiperparámetros\n",
    "        # epochs\n",
    "        n_epochs = [25, 50]\n",
    "        # learning rates\n",
    "        learning_rates = [0.001, 0.0005, 0.0001, 0.00005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_EfficientNet_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history4 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo EfficientNet B0...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_EfficientNet()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_EfficientNet_binary()\n",
    "                if CF5_flag:\n",
    "                    model = build_model_EfficientNet_dropout()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])    \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, train_dataset, valid_dataset, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)                    \n",
    "                # Adición de los resultados al diccionario history4\n",
    "                history4['epochs'].append(epochs)\n",
    "                history4['batch_size'].append(batch_size)\n",
    "                history4['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history4['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history4['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history4['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history4['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history4['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(input_path + 'df_histfit4')\n",
    "                    model.save(input_path + 'efficientNet_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history4 = pd.DataFrame.from_dict(history4)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history4.to_pickle(input_path + 'df_history4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "df_history4 = pd.read_pickle(input_path + 'df_history4')\n",
    "\n",
    "# Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "df_history4.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "df_histfit4 = pd.read_pickle(input_path + 'df_histfit4')\n",
    "\n",
    "# Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "plot_acc_loss(df_histfit4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_EfficientNet_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_EfficientNet()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_EfficientNet_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_EfficientNet_dropout()\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(input_path + 'efficientNet_best_model.h5')\n",
    "    model.save(input_path + 'model4.h5') # Guardado completo del modelo\n",
    "\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del modelo completo\n",
    "if CF1_flag or CF4_flag:\n",
    "    model = build_model_EfficientNet()\n",
    "if CF2_flag or CF3_flag:\n",
    "    model = build_model_EfficientNet_binary()\n",
    "if CF5_flag:\n",
    "    model = build_model_EfficientNet_dropout()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.load_weights(input_path + 'model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción con el conjunto de test\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC multiclase\n",
    "plot_multiclass_roc(model, x_test, y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "\n",
    "# Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "#plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print('***** MÉTRICAS *****')\n",
    "print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "if CF1_flag or CF4_flag or CF5_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "if CF2_flag or CF3_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "for i in range(len(classes)):\n",
    "    m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "    \n",
    "# Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "m1 = m1.append(m_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Modelo-5-%C2%B7-Red-Xception\">Modelo 5 · Red Xception<a class=\"anchor-link\" href=\"#Modelo-5-%C2%B7-Red-Xception\">¶</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de los conjuntos de train y test\n",
    "x_train = np.load(input_path + 'x_train.npy')\n",
    "x_test = np.load(input_path + 'x_test.npy')\n",
    "y_train = np.load(input_path + 'y_train.npy')\n",
    "y_test = np.load(input_path + 'y_test.npy')\n",
    "x_val = np.load(input_path + 'x_val.npy')\n",
    "y_val = np.load(input_path + 'y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_Xception():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = Xception(include_top = False,                         # no incluir la capa de clasificación\n",
    "                           input_tensor = inputs, \n",
    "                           weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_Xception_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = Xception(include_top = False,                         # no incluir la capa de clasificación\n",
    "                           input_tensor = inputs, \n",
    "                           weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(2, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "\n",
    "def build_model_Xception_dropout():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = Xception(include_top = False,                         # no incluir la capa de clasificación\n",
    "                           input_tensor = inputs, \n",
    "                           weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5), \n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_Xception()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_Xception_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_Xception_dropout()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32, 64, 128]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    if CF5_flag:\n",
    "        # Hiperparámetros\n",
    "        # epochs\n",
    "        n_epochs = [25, 50]\n",
    "        # learning rates\n",
    "        learning_rates = [0.001, 0.0005, 0.0001, 0.00005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history5 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo Xception...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_Xception()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_Xception_binary()\n",
    "                if CF5_flag:\n",
    "                    model = build_model_Xception_dropout()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])       \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, train_dataset, valid_dataset, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)            \n",
    "                # Adición de los resultados al diccionario history5\n",
    "                history5['epochs'].append(epochs)\n",
    "                history5['batch_size'].append(batch_size)\n",
    "                history5['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history5['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history5['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history5['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history5['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history5['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(input_path + 'df_histfit5')\n",
    "                    model.save(input_path + 'Xception_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history5 = pd.DataFrame.from_dict(history5)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history5.to_pickle(input_path + 'df_history5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "df_history5 = pd.read_pickle(input_path + 'df_history5')\n",
    "\n",
    "# Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "df_history5.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "df_histfit5 = pd.read_pickle(input_path + 'df_histfit5')\n",
    "\n",
    "# Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "plot_acc_loss(df_histfit5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_xception_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_Xception()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_Xception_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_Xception_dropout()\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(input_path + 'Xception_best_model.h5')\n",
    "    model.save(input_path + 'model5.h5') # Guardado completo del modelo\n",
    "\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CF1_flag or CF4_flag:\n",
    "    model = build_model_Xception()\n",
    "if CF2_flag or CF3_flag:\n",
    "    model = build_model_Xception_binary()\n",
    "if CF5_flag:\n",
    "    model = build_model_Xception_dropout()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.load_weights(input_path + 'model5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción con el conjunto de test\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC multiclase\n",
    "plot_multiclass_roc(model, x_test, y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "\n",
    "# Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "#plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print('***** MÉTRICAS *****')\n",
    "print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "if CF1_flag or CF4_flag or CF5_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "if CF2_flag or CF3_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "for i in range(len(classes)):\n",
    "    m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "    \n",
    "# Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "m1 = m1.append(m_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Modelo-6-%C2%B7-ResNet152\">Modelo 6 · ResNet152<a class=\"anchor-link\" href=\"#Modelo-6-%C2%B7-ResNet152\">¶</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de los conjuntos de train y test\n",
    "x_train = np.load(input_path + 'x_train.npy')\n",
    "x_test = np.load(input_path + 'x_test.npy')\n",
    "y_train = np.load(input_path + 'y_train.npy')\n",
    "y_test = np.load(input_path + 'y_test.npy')\n",
    "x_val = np.load(input_path + 'x_val.npy')\n",
    "y_val = np.load(input_path + 'y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_ResNet152():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = ResNet152(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_ResNet152_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = ResNet152(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(2, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "\n",
    "def build_model_ResNet152_dropout():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = ResNet152(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5), \n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_ResNet152()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_ResNet152_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_ResNet152_dropout()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32]\n",
    "    # learning rates\n",
    "    learning_rates = [0.0005, 0.0001, 0.00005]\n",
    "    if CF5_flag:\n",
    "        # Hiperparámetros\n",
    "        # epochs\n",
    "        n_epochs = [25, 50]\n",
    "        # learning rates\n",
    "        learning_rates = [0.0005, 0.0001, 0.00005, 0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history6 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo ResNet152...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_ResNet152()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_ResNet152_binary()\n",
    "                if CF5_flag:\n",
    "                    model = build_model_ResNet152_dropout()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])   \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, train_dataset, valid_dataset, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)      \n",
    "\n",
    "                # Adición de los resultados al diccionario history6\n",
    "                history6['epochs'].append(epochs)\n",
    "                history6['batch_size'].append(batch_size)\n",
    "                history6['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history6['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history6['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history6['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history6['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history6['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(input_path + 'df_histfit6')\n",
    "                    model.save(input_path + 'ResNet152_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history6 = pd.DataFrame.from_dict(history6)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history6.to_pickle(input_path + 'df_history6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "df_history6 = pd.read_pickle(input_path + 'df_history6')\n",
    "\n",
    "# Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "df_history6.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "df_histfit6 = pd.read_pickle(input_path + 'df_histfit6')\n",
    "\n",
    "# Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "plot_acc_loss(df_histfit6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_resnet_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_ResNet152()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_ResNet152_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_ResNet152_dropout()\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(input_path + 'ResNet152_best_model.h5')\n",
    "    model.save(input_path + 'model6.h5') # guardado del modelo completo\n",
    "    \n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del modelo completo\n",
    "if CF1_flag or CF4_flag:\n",
    "    model = build_model_ResNet152()\n",
    "if CF2_flag or CF3_flag:\n",
    "    model = build_model_ResNet152_binary()\n",
    "if CF5_flag:\n",
    "    model = build_model_ResNet152_dropout()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.load_weights(input_path + 'model6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción con el conjunto de test\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC multiclase\n",
    "plot_multiclass_roc(model, x_test, y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "\n",
    "# Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "#plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print('***** MÉTRICAS *****')\n",
    "print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "if CF1_flag or CF4_flag or CF5_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "if CF2_flag or CF3_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "for i in range(len(classes)):\n",
    "    m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "    \n",
    "# Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "m1 = m1.append(m_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Modelo-7-%C2%B7-VGG16\">Modelo 7 · VGG16<a class=\"anchor-link\" href=\"#Modelo-7-%C2%B7-VGG16\">¶</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de los conjuntos de train y test\n",
    "x_train = np.load(input_path + 'x_train.npy')\n",
    "x_test = np.load(input_path + 'x_test.npy')\n",
    "y_train = np.load(input_path + 'y_train.npy')\n",
    "y_test = np.load(input_path + 'y_test.npy')\n",
    "x_val = np.load(input_path + 'x_val.npy')\n",
    "y_val = np.load(input_path + 'y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_VGG16():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = VGG16(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_VGG16_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = VGG16(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(2, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "\n",
    "def build_model_VGG16_dropout():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = VGG16(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_VGG16()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_VGG16_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_VGG16_dropout()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32, 64, 128]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    if CF5_flag:\n",
    "        # Hiperparámetros\n",
    "        # epochs\n",
    "        n_epochs = [25, 50]\n",
    "        # learning rates\n",
    "        learning_rates = [0.001, 0.0005, 0.0001, 0.00005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history7 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo VGG16...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_VGG16()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_VGG16_binary()\n",
    "                if CF5_flag:\n",
    "                    model = build_model_VGG16_dropout()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])        \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, train_dataset, valid_dataset, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)         \n",
    "                # Adición de los resultados al diccionario history7\n",
    "                history7['epochs'].append(epochs)\n",
    "                history7['batch_size'].append(batch_size)\n",
    "                history7['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history7['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history7['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history7['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history7['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history7['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(input_path + 'df_histfit7')\n",
    "                    model.save(input_path + 'VGG16_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history7 = pd.DataFrame.from_dict(history7)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history7.to_pickle(input_path + 'df_history7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "df_history7 = pd.read_pickle(input_path + 'df_history7')\n",
    "\n",
    "# Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "df_history7.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "df_histfit7 = pd.read_pickle(input_path + 'df_histfit7')\n",
    "\n",
    "# Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "plot_acc_loss(df_histfit7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_VGG16_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_VGG16()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_VGG16_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_VGG16_dropout()\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(input_path + 'VGG16_best_model.h5')\n",
    "    model.save(input_path + 'model7.h5') # guardado del modelo completo\n",
    "\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del modelo completo\n",
    "if CF1_flag or CF4_flag:\n",
    "    model = build_model_VGG16()\n",
    "if CF2_flag or CF3_flag:\n",
    "    model = build_model_VGG16_binary()\n",
    "if CF5_flag:\n",
    "    model = build_model_VGG16_dropout()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.load_weights(input_path + 'model7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción con el conjunto de test\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC multiclase\n",
    "plot_multiclass_roc(model, x_test, y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "\n",
    "# Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "#plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print('***** MÉTRICAS *****')\n",
    "print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "if CF1_flag or CF4_flag or CF5_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "if CF2_flag or CF3_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "for i in range(len(classes)):\n",
    "    m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "    \n",
    "# Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "m1 = m1.append(m_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Modelo-8-%C2%B7-DenseNet201\">Modelo 8 · DenseNet201<a class=\"anchor-link\" href=\"#Modelo-8-%C2%B7-DenseNet201\">¶</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de los conjuntos de train y test\n",
    "x_train = np.load(input_path + 'x_train.npy')\n",
    "x_test = np.load(input_path + 'x_test.npy')\n",
    "y_train = np.load(input_path + 'y_train.npy')\n",
    "y_test = np.load(input_path + 'y_test.npy')\n",
    "x_val = np.load(input_path + 'x_val.npy')\n",
    "y_val = np.load(input_path + 'y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_DenseNet201():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = DenseNet201(include_top = False,                   # no incluir la capa de clasificación\n",
    "                        input_tensor = inputs,\n",
    "                        weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_DenseNet201_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = DenseNet201(include_top = False,                   # no incluir la capa de clasificación\n",
    "                        input_tensor = inputs,\n",
    "                        weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(2, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_DenseNet201_dropout():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = DenseNet201(include_top = False,                   # no incluir la capa de clasificación\n",
    "                        input_tensor = inputs,\n",
    "                        weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_DenseNet201()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_DenseNet201_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_DenseNet201_dropout()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 12, 16]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    if CF5_flag:\n",
    "        # Hiperparámetros\n",
    "        # epochs\n",
    "        n_epochs = [25, 50]\n",
    "        # learning rates\n",
    "        learning_rates = [0.001, 0.0005, 0.0001, 0.00005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history8 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo DenseNet201...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_DenseNet201()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_DenseNet201_binary()\n",
    "                if CF5_flag:\n",
    "                    model = build_model_DenseNet201_dropout()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])        \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, train_dataset, valid_dataset, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)            \n",
    "                # Adición de los resultados al diccionario history8\n",
    "                history8['epochs'].append(epochs)\n",
    "                history8['batch_size'].append(batch_size)\n",
    "                history8['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history8['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history8['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history8['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history8['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history8['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(input_path + 'df_histfit8')\n",
    "                    model.save(input_path + 'DenseNet201_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history8 = pd.DataFrame.from_dict(history8)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history8.to_pickle(input_path + 'df_history8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "df_history8 = pd.read_pickle(input_path + 'df_history8')\n",
    "\n",
    "# Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "df_history8.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "df_histfit8 = pd.read_pickle(input_path + 'df_histfit8')\n",
    "\n",
    "# Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "plot_acc_loss(df_histfit8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_densenet_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_DenseNet201()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_DenseNet201_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_DenseNet201_dropout()\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(input_path + 'DenseNet201_best_model.h5')\n",
    "    model.save(input_path + 'model8.h5') # guardado del modelo completo\n",
    "    \n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del modelo completo\n",
    "if CF1_flag or CF4_flag:\n",
    "    model = build_model_DenseNet201()\n",
    "if CF2_flag or CF3_flag:\n",
    "    model = build_model_DenseNet201_binary()\n",
    "if CF5_flag:\n",
    "    model = build_model_DenseNet201_dropout()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.load_weights(input_path + 'model8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción con el conjunto de test\n",
    "y_pred = model.predict(x_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC multiclase\n",
    "plot_multiclass_roc(model, x_test, y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "\n",
    "# Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "#plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print('***** MÉTRICAS *****')\n",
    "print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "if CF1_flag or CF4_flag or CF5_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "if CF2_flag or CF3_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "for i in range(len(classes)):\n",
    "    m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "    \n",
    "# Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "m1 = m1.append(m_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Modelo-9-%C2%B7-MobileNetV2\">Modelo 9 · MobileNetV2<a class=\"anchor-link\" href=\"#Modelo-9-%C2%B7-MobileNetV2\">¶</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de los conjuntos de train y test\n",
    "x_train = np.load(input_path + 'x_train.npy')\n",
    "x_test = np.load(input_path + 'x_test.npy')\n",
    "y_train = np.load(input_path + 'y_train.npy')\n",
    "y_test = np.load(input_path + 'y_test.npy')\n",
    "x_val = np.load(input_path + 'x_val.npy')\n",
    "y_val = np.load(input_path + 'y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_MobileNetV2():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = MobileNetV2(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_MobileNetV2_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = MobileNetV2(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(2, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "\n",
    "def build_model_MobileNetV2_dropout():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = MobileNetV2(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5), \n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_MobileNetV2()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_MobileNetV2_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_MobileNetV2_dropout()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32, 64, 128]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    if CF5_flag:\n",
    "        # Hiperparámetros\n",
    "        # epochs\n",
    "        n_epochs = [25, 50]\n",
    "        # learning rates\n",
    "        learning_rates = [0.001, 0.0005, 0.0001, 0.00005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history9 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo MobileNetV2...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_MobileNetV2()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_MobileNetV2_binary()\n",
    "                if CF5_flag:\n",
    "                    model = build_model_MobileNetV2_dropout()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])       \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, train_dataset, valid_dataset, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)            \n",
    "                # Adición de los resultados al diccionario history9\n",
    "                history9['epochs'].append(epochs)\n",
    "                history9['batch_size'].append(batch_size)\n",
    "                history9['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history9['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history9['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history9['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history9['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history9['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(input_path + 'df_histfit9')\n",
    "                    model.save(input_path + 'MobileNetV2_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history9 = pd.DataFrame.from_dict(history9)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history9.to_pickle(input_path + 'df_history9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "df_history9 = pd.read_pickle(input_path + 'df_history9')\n",
    "\n",
    "# Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "df_history9.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "df_histfit9 = pd.read_pickle(input_path + 'df_histfit9')\n",
    "\n",
    "# Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "plot_acc_loss(df_histfit9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_mobilenet_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_MobileNetV2()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_MobileNetV2_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_MobileNetV2_dropout()\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(input_path + 'MobileNetV2_best_model.h5')\n",
    "    model.save(input_path + 'model9.h5') # guardado del modelo completo\n",
    "\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del modelo completo\n",
    "if CF1_flag or CF4_flag:\n",
    "    model = build_model_MobileNetV2()\n",
    "if CF2_flag or CF3_flag:\n",
    "    model = build_model_MobileNetV2_binary()\n",
    "if CF5_flag:\n",
    "    model = build_model_MobileNetV2_dropout()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.load_weights(input_path + 'model9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción con el conjunto de test\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC multiclase\n",
    "plot_multiclass_roc(model, x_test, y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "\n",
    "# Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "#plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print('***** MÉTRICAS *****')\n",
    "print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "if CF1_flag or CF4_flag or CF5_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "if CF2_flag or CF3_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "for i in range(len(classes)):\n",
    "    m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "    \n",
    "# Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "m1 = m1.append(m_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Modelo-10-%C2%B7-InceptionResNetV2\">Modelo 10 · InceptionResNetV2<a class=\"anchor-link\" href=\"#Modelo-10-%C2%B7-InceptionResNetV2\">¶</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de los conjuntos de train y test\n",
    "x_train = np.load(input_path + 'x_train.npy')\n",
    "x_test = np.load(input_path + 'x_test.npy')\n",
    "y_train = np.load(input_path + 'y_train.npy')\n",
    "y_test = np.load(input_path + 'y_test.npy')\n",
    "x_val = np.load(input_path + 'x_val.npy')\n",
    "y_val = np.load(input_path + 'y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_InceptionResNetV2():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = InceptionResNetV2(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_InceptionResNetV2_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = InceptionResNetV2(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(2, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "\n",
    "def build_model_InceptionResNetV2_dropout():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = InceptionResNetV2(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.5), \n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_InceptionResNetV2()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_InceptionResNetV2_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_InceptionResNetV2_dropout()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32, 64, 128]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    if CF5_flag:\n",
    "        # Hiperparámetros\n",
    "        # epochs\n",
    "        n_epochs = [25, 50]\n",
    "        # learning rates\n",
    "        learning_rates = [0.001, 0.0005, 0.0001, 0.00005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history10 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo InceptionResNetV2...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_InceptionResNetV2()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_InceptionResNetV2_binary()\n",
    "                if CF5_flag:\n",
    "                    model = build_model_InceptionResNetV2_dropout()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])       \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, train_dataset, valid_dataset, batch_size, epochs,'+\n",
    "                                                 'callbacks)', globals = globals(), number = 1)            \n",
    "                # Adición de los resultados al diccionario history10\n",
    "                history10['epochs'].append(epochs)\n",
    "                history10['batch_size'].append(batch_size)\n",
    "                history10['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history10['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history10['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history10['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history10['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history10['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(input_path + 'df_histfit10')\n",
    "                    model.save(input_path + 'InceptionResNetV2_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history10 = pd.DataFrame.from_dict(history10)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history10.to_pickle(input_path + 'df_history10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "df_history10 = pd.read_pickle(input_path + 'df_history10')\n",
    "\n",
    "# Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "df_history10.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "df_histfit10 = pd.read_pickle(input_path + 'df_histfit10')\n",
    "\n",
    "# Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "plot_acc_loss(df_histfit10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_InceptionResNetV2_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_InceptionResNetV2()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_InceptionResNetV2_binary()\n",
    "    if CF5_flag:\n",
    "        model = build_model_InceptionResNetV2_dropout()\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(input_path + 'InceptionResNetV2_best_model.h5')\n",
    "    model.save(input_path + 'model10.h5') # guardado del modelo completo\n",
    "\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del modelo completo\n",
    "if CF1_flag or CF4_flag:\n",
    "    model = build_model_InceptionResNetV2()\n",
    "if CF2_flag or CF3_flag:\n",
    "    model = build_model_InceptionResNetV2_binary()\n",
    "if CF5_flag:\n",
    "    model = build_model_InceptionResNetV2_dropout()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.load_weights(input_path + 'model10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción con el conjunto de test\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC multiclase\n",
    "plot_multiclass_roc(model, x_test, y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "\n",
    "# Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "#plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print('***** MÉTRICAS *****')\n",
    "print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "if CF1_flag or CF4_flag or CF5_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "if CF2_flag or CF3_flag:\n",
    "    m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "for i in range(len(classes)):\n",
    "    m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "    \n",
    "# Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "m1 = m1.append(m_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinicio de los índices\n",
    "m1 = m1.reset_index()\n",
    "# Borrado de la columna index\n",
    "m1 = m1.drop(['index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_matriz_pesos_flag == True:\n",
    "    # Guardado de la matriz de pesos de los modelos\n",
    "    m1.to_pickle(input_path + 'm1')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4057513,
     "sourceId": 7154484,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
